---
title: "Regression Project"
author: "Group 8"
format: html
editor: 
  markdown: 
    wrap: sentence
---

## Introduction

For decades, a secondary education has been considered the gold standard for ensuring a comfortable career and wage.
In recent years however, it is becoming more and more difficult for recent graduates to find careers that match up with their major of study, and therefore the wages that go with it.
We found this topic important to study due to the prevalence of career prospects in our own lives as well as society at large.
Our data is from an inbuilt RStudio package, causaldata, and has 30401 observations, omitting NAs.
Our outcome of interest is median income (earnings_med), while our predictors of interest are degree awarded (pred_degree_awarded_ipeds), working (count_working), not working (count_not_working), year, and region.
Degree awarded has 3 different observations: 1 being a less-than-two-year degree, 2 being a two-year degree, and 3 being a four-year degree or more.
The year variable goes from 2007 until 2014.
The region variable is broken down into Northeast, North Central, South, and West.

```{r}
# all of our packages
library(causaldata)
library(datasets)
library(tidyverse)
library(ggfortify)
library(MASS)
library(pheatmap)
library(car)
library(lmtest)
library(splines)
library(reshape2)
library(interactions)
library(boot)
library(simpleboot)
library(emmeans)

# the functions used in multiple places
predict_loo <- function(model) {
y <- model.frame(model)[,1]
loo_r <- residuals(model) / (1 - hatvalues(model))
return(y - loo_r)
}
rsq_loo <- function(model) {
y <- model.frame(model)[,1]
yhat <- predict_loo(model)
return(cor(y, yhat)^2)
}
p_print <- function(object){
  print(deparse(substitute(object)))
  print(object)
}
as.numeric.l <- function(list){
  if(is.factor(list)){
    list <- as.numeric(list)
  }
  return(list)
}

## cleaning
# making region match which region the state is in
scorecard$region <- state.region[match(scorecard$state_abbr, state.abb)]
# making the degree a factor
scorecard$degree <- as.factor(scorecard$pred_degree_awarded_ipeds)
scorecard <- na.omit(scorecard)
```

## Research Questions

#### 1. Does median income have a positive relationship with the proportion of working graduates?

-   $H_0$: Median income will have a positive relationship with the number of working graduates.

-   $H_A$: positive relationship and significant p-value will prove this to be true.

#### 2. Which US region contributes most to median earnings?

-   $H_0$: All regions do not differ significantly for median earnings

-   $H_A$: Eastern region will be the most significant in median earnings compared to other regions.

#### 3. Which degree length leads to higher median salary?

-   $H_0$: Median salary does not significantly differ between degree lengths.

-   $H_A$: People with 4 year degrees have higher median salaries compared to other degre

## Data Exploration

The purpose of this project is to analyze a potential relationship between college graduates in the US and median earnings based on year, region, degree length, and working status. Firstly, NA values were removed from the data set. 
States were grouped into regions using the state.region() base R function, and the degree length was converted to a factor with levels for simplified analysis. 
Additional tidying was performed utilizing gather(), where “employment_status” was created with the paired variable “working” listing the number of graduates either currently working or not working. The following is a brief analysis of trends among selected predictors (year, employment_status (count_working and count_not_working), degree, and region) and median earnings (earnings_med). 

#### Manipulate Data

```{r}
# Organize states into regions 
scorecard$region <- state.region[match(scorecard$state_abbr, state.abb)]
scorecard <- na.omit(scorecard)

# Change variable name to 'degree'
scorecard = scorecard %>%
  mutate(degree = as.factor(pred_degree_awarded_ipeds))

glimpse(scorecard)
```

#### Region Exploration
A constructed boxplot illustrates trends in median earnings for graduates based on the US region. Graduates situated in the Northeastern US have the highest median of earnings while the South has the lowest median earning. 
Both the north central and western US have similar median earnings; however, western and northeastern states have a much wider distribution of earnings than southern and the north central states. 
Northeastern states may have higher median earnings because of an increased concentration of “prestigious” universities (ex. Ivy League) in this region; there may be a “higher” quality of education that is more sought after for employment. This observation, however, requires additional investigation.


```{r}
# Plot for Graduate Median Earnings Based on Region
scorecard %>% ggplot(aes(x = region, y = earnings_med, fill = region)) +
  geom_boxplot() +
  labs(title = "Graduate Median Earnings Based on Region",
       x = "Region",
       y = "Median Earnings" ) +
  scale_fill_brewer(palette = "Set2") +  
  theme_minimal()

# Plot for Median Earnings Based on Region
scorecard %>% 
  ggplot(aes(x = earnings_med, fill = region)) +
  geom_histogram(bins = 35, col = "black") + facet_grid(~region) +
  labs(title = "Median Earnings Based on Region",
       x = "Median Earnings") + facet_grid(~region) + theme_minimal()

```

#### Employment Status vs. Median Earnings

A scatterplot was utilized to determine the distribution of median earnings for graduates based on working status. As seen in the rightmost scatterplot, there are more working graduates than those who do not work; there are notable extremes for count_working, where at least two data points indicate more than 7,500 working graduates were earning. 
Working graduates seem more numerous and spread out for median earnings compared to those who did not work. 
Surprisingly, working and non-working graduates seem to have similarly distributed median earnings; this goes against an assumption that working graduates would have higher median salaries than those who do not work. 

```{r}
# Create a new dataframe with tidy long performed on status of work
scorecard1 <- scorecard %>% gather(key = "employment_status", value = "working", c(count_not_working, count_working)) 

scorecard1 <- scorecard1 %>% mutate(employment_status = as.factor(employment_status))

# Plot for Median Earnings of Graduates Based on Employment Status
scorecard1 %>% 
  ggplot(aes(x = working, y = earnings_med, color = employment_status)) +
  geom_point() +   facet_grid(~employment_status) +
  labs(title = "Median Earnings of Graduates Based on Employment Status",
       x = "Number of Graduates",
       y = "Median Earnings") +
  scale_color_brewer(palette = "Dark2", name = "Employment Status", labels = c("Not Working", "Working")) +
  theme_minimal() +
  theme(legend.position = "top") +
  geom_smooth(method = "lm", linetype = "dashed", color = "black") 

```

#### Degree Length vs. Median Earnings
It was expected that the longer the degree length, the higher the median earnings for graduates. Those who completed an undergraduate degree or pursued further education (ex. graduate school) earned much more than those who did not; the median earnings for this group are higher, and there is more spread for median earnings. This visually supports the observation that graduates with at least an undergraduate degree or additional completed schooling have a higher chance of employment because academic and professional skills may be more developed for a selected profession. As such, preference may be given for graduates with an undergraduate degree or higher education status compared to those who have two or less years of completed education. 

```{r}
# Plot for Median Earnings of Graduates Based on Degree Length
scorecard1 %>% 
  ggplot(aes(x = degree, y = earnings_med, fill = degree)) +
  geom_boxplot() +
  labs(title = "Median Earnings of Graduates Based on Degree Length",
       x = "Degree Length", y = "Median Earnings") +  theme_minimal() + scale_fill_discrete(labels=c('Less than 2 years', 'Two years', '4+ years')) + 
  scale_x_discrete(labels = c('Less than 2 years', 'Two years', '4+ years'))
```

#### Year vs. Median Earnings
Median earnings based on years did not frequently change, and years 2009 and 2011 saw at least one visible outlier among graduates for more than $150,000 earned. Because yearly data for 2008 and post 2014 was omitted due to missing information, the yearly earnings distribution may not be reflective of additional years. 
However, the trends in the boxplot indicate that median earnings generally decreased after 2009; this may indicate a response to economic effects. Additional research is required to further investigate this observation.

```{r}
# changing year to a factor
scorecard1$year_level <- as.factor(scorecard1$year)

# Plot for Median Earnings of Graduates 2007 - 2014
scorecard1 %>% 
  ggplot(aes(x = year_level, y = earnings_med, fill = year_level)) +
  geom_boxplot() +
  labs(title = "Median Earnings of Graduates 2007 - 2014",
       x = "Year", y = "Median Earnings") +  theme_minimal()
```

#### Correlations
A correlation matrix was generated to visualize correlations between selected numeric variables.The highest correlation was found to be between `count_working` and `count_not_working` at 0.97. This is trivial since the number of graduates that are able to find work directly influences the number of graduates that remained unemployed. The magnitudes of all other correlation factors were below 0.25, suggesting the variables have a quite weak relationship.

```{r}
# getting year as a numeric
scorecard$year_num <- as.numeric(scorecard$year)

# getting the heat map for the year, earnings_med, count_not_working, count_working
scorecard_cor <- cor(na.omit(scorecard[,c(5, 6, 7, 8)]))
pheatmap(scorecard_cor,
        treeheight_col = 0,
        treeheight_row = 0,
        display_numbers = TRUE,
        breaks = seq(-1, 1, length = 101))
```

## Multiple Linear Regression Model

To assess for the presence of a predictive relationship between the median earnings of individuals graduating from colleges and universities across the United States and characteristics associated with their alma mater and post college lives, we constructed a linear model regressing median earnings on surveyed universities' regional location, the number of alumni both employed and not working (not necessarily un-employed), the primary degree awarded, and the year that each survey was conducted.

```{r}
earnings_lm<-lm(earnings_med~region+degree+year+count_not_working+count_working, data=scorecard)
summary(earnings_lm)
scorecard$region<-relevel(scorecard$region, ref="Northeast")
cat("Earnings median range:", range(scorecard$earnings_med))
```

The substantial F-statistic generated by the linear model of 3691 on 8 and 30392 degrees of freedom allowed us to reject the null hypothesis that none of the chosen variables possess any relationship to median earnings (all slopes are equal to zero) in favor of the alternative hypothesis that at least one of the predictive variables influences the earnings of American college graduates (at least one slope is not equal to zero).
Given the confirmation of, at minimum, one of our independent variables' predictive power, we further explored the more nuanced ways in which each contributed to variation from the baseline predicted income of \$503,340, as denoted by the intercept regression coefficient.
Holding the influence of region, degrees typically awarded, year, and the number of graduates not actively employed constant, a one person increase in the number of gainfully employed graduates contributed to an institution results in a marginal \$1.56 increase in predicted median earnings.
Conversely, when controlling for the effect of all other predictors, the addition of a single non-working alumni unsurprisingly elicits a predicted \$8.86 decline in predicted income.
Assessment of the regression coefficient assigned to the year variable in the same manner revealed a slightly more impactful association between the year participants were surveyed and median earnings, with the passage of one year resulting in a loss of \$235.90.
Due to the categorical nature of the predominant degree awarded by collegiate study participants and the region in which each institution of higher learning resides, the analysis of their influence on predicted monetary outcomes diverged from that of aforementioned variables.
As a hub for a variety of prestigious Universities, we anticipated that graduates from Northeastern schools would likely possess the highest median earnings and we accordingly designated it as the reference for our analysis of regional impacts.
When controlling for the effects of all other variables and regions, prior attendance of a Southern school resulted in an average median earnings reduction of \$4,613 from the Northeastern baseline.
Upon similar evaluation, graduation from North Central and Western colleges comparably resulted in an average loss of \$3,651 and \$1,761, respectively.
In considering the impact of the predominant degree awarded we identified the widest range of variation between predicted monetary outcomes, with the reference of less than 2 years differing by ampler amounts than the deviations observed between the regional categories.
Controlling for all other variables and education levels, completion of a 2 year degree improved average predicted median income by \$5,892, while graduation with a bachelor's degree raised income by an average of \$15,670 after comparison to the baseline.
Though all of the regression coefficients for both numeric and categorical variables possessed p-values significant at the zero level (p \<2\*10-16), the multiple R2 value of 0.498 indicates that only approximately 50% of the variation observed in median earnings for those surveyed is accounted for by the collegiate attributes analyzed above.
This is reflected by the substantial residual standard error of 8373 on 30,392 degrees of freedom, meaning that the predicted values produced by the linear model deviate from actual monetary outcomes by an average of \$8373.
When compared to both the regression coefficients and the overall range of the actual median earnings values (\$8604-\$171900), the level of error observed in the estimates produced by the model is concerning and likely indicative of improper model fit through overfitting or multicollinearity.

## Improving the Model

#### Setting Up to Analyze the Models

```{r}
results <- data.frame()
analyze_model <- function(model){
  # testing the if errors zero on-average
  # closer to zero is good
  resid_avg_zero_test <- t.test(resid(model), mu=0)
  
  # testing for constant variance
  # closer to zero is better 
  heteroscedasticity_test <- bptest(model)
  
  # used for checking for overfitting
  #higher LOOR2 is better but R2 being much greater than LOOR2 incdicates overfitting
  LOOR2 <- rsq_loo(model)
  R2 <- summary(model)[["r.squared"]]
  
  
  # Ensure the types are compatible
  model_call_str <- as.character(paste(deparse(model$call), collapse = " "))
  resid_statistic <- as.numeric(resid_avg_zero_test$statistic)
  errors_zero_is_pass <-resid_avg_zero_test$p.value > .05
  bp_statistic <- as.numeric(heteroscedasticity_test$statistic)
  constant_variance_is_pass <- heteroscedasticity_test$p.value > .05
  loor2_value <- as.numeric(LOOR2)
  r2_value <- as.numeric(R2)

  # Append the new row 
  new_row <- data.frame(model=model_call_str, t=resid_statistic, errors_zero_is_pass, bp=bp_statistic, constant_variance_is_pass=constant_variance_is_pass,  R2=r2_value, LOOR2=loor2_value)
  
  # Making sure that the results are actually getting added
  results <<- rbind(results, new_row)
}
```
#### Diagnostic Plots

```{r}
analyze_model(earnings_lm)
print(results[NROW(results),]) # !!! TODO reformat this so it is easier to interpret
```

According to the T-test for mean of average zero, the errors appear to be zero on-average for all fitted values. The value of T is extremely close to zero, which indicates a p-value close to zero and therefore is not significant evidence that the mean varies from zero. According to the Breusch-Pagan test, the errors do not have constant variance and are heteroskedastic with a bp of 855.68, which would led to rejecting the test. This indicates that we must reject the null hypothesis. The normality is not passible, with points at the higher quantiles doubling the expected values.

#### Transformations

```{r}
log_model <- lm(log(earnings_med) ~ region + degree + year + count_not_working + 
    count_working, data = scorecard)
analyze_model(log_model)
results[NROW(results),]# !!! TODO reformat this so it is easier to interpret
```

We used a log-dependent transformation. According to the T-test for mean of average zero, the errors appear to be zero on average. According to the Breusch-Pagan test, the errors do not have constant variance. However the BP is worse than that of the previous model. The normality is better than the previous model as the qq-plot shows less violation of the assumption.

#### Evidence Supporting the Inclusion of A Spline

```{r}
for(k in 1:5){
  formula_str <- sprintf("lm(earnings_med ~ region + degree + year + ns(count_not_working, df=%d) + ns(count_working, df=%d), data=scorecard)", k,k)
  spine_both_model <- eval(parse(text = formula_str))
  analyze_model(spine_both_model)
  
  formula_str <- sprintf("lm(earnings_med ~ region + degree + year + ns(count_not_working*count_working, df=%d), data=scorecard)", k)
  spine_both_int_model <- eval(parse(text = formula_str))
  analyze_model(spine_both_model)
  
  
  formula_str <- sprintf("lm(log(earnings_med) ~ region + degree + year + ns(count_not_working, df=%d) + ns(count_working, df=%d), data=scorecard)", k,k)
  spine_both_model <- eval(parse(text = formula_str))
  analyze_model(spine_both_model)
  
  formula_str <- sprintf("lm(log(earnings_med) ~ region + degree + year + ns(count_not_working*count_working, df=%d), data=scorecard)", k)
  spine_both_int_model <- eval(parse(text = formula_str))
  analyze_model(spine_both_model)
}
```

None of the splines have a significantly good impact on the model, so we will not include one in our model !!! TODO this is a lie??? because i was being dumb

#### Interpret Interaction
There is likely an interaction between count_not_working, count_working, and year.

```{R}
count_year_int_model <- lm(earnings_med ~ region + degree + (count_not_working + count_working) * year, data=scorecard)
summary(count_year_int_model)
analyze_model(count_year_int_model)
anova(earnings_lm, count_year_int_model)
drop1(count_year_int_model, test = "F") # TODO FIX FORMATTING FOR THIS AREA
```

The model predicts that for every 1 increase in count_not_working that the effect of year will increase 6.179e-01 units. The model also predicts for every 1 unit increase in count_working that the effect of year will decrease 7.314e-02 units.

```{r}
results <- na.omit(results)
results$row_names <- row.names(results)
results <- results[order(-results$LOOR2), ]
results$row_names <- factor(results$row_names, levels = results$row_names)

# Plot for 't'
ggplot(results, aes(x = row_names, y = t)) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("t values")


# Create a transformation for 'LOOR2'
max_bp <- max(results$bp, na.rm = TRUE)
max_LOOR2 <- max(results$LOOR2, na.rm = TRUE)
scale_factor <- max_bp / max_LOOR2

# Apply the unified scaling to both LOOR2 and R2
results$LOOR2_scaled <- results$LOOR2 * scale_factor

# Update melting to include both scaled LOOR2 and R2
long_results <- melt(results, id.vars = "row_names", measure.vars = c("bp", "LOOR2_scaled"))

# Update variable names for the legend
long_results$variable <- factor(long_results$variable, labels = c("BP", "Scaled LOOR2"))

# Plot
ggplot(long_results, aes(x = row_names, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +
  scale_fill_manual(values = c("BP" = "#87CEFA", "Scaled LOOR2" = "#FF6A6A")) +
  scale_y_continuous("BP", sec.axis = sec_axis(~ . / scale_factor, name = "Scaled LOOR2")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("BP and Scaled LOOR2 for each model") +
  labs(x = "Row Names", y = "BP Value", fill = "Variable")

results[1,]
best_model <- eval(parse(text=results[1,"model"]))
summary(best_model)
autoplot(best_model)
```

## Formal Hypothesis Tests

To begin the project, we wanted to investigate how the number of working graduates (`count_working`), region of the university (`region`), and degree type (`degree`) relate to the median income of graduates (`earnings_med`).
In this section, we tested if each of these three variables are significant in predicting the median income.
Firstly, we used the following equation to represent the relationship between median earnings and the chosen predictors: $$
Y = \beta_0 + \beta_{r_1}X_{r_1} + \beta_{r_2}X_{r_2} + \beta_{r_3}X_{r_3} + \beta_{d_1}X_{d_2} + \beta_{d_2}X_{d_2} + \beta_yX_y + \beta_nX_n + \beta_wX_w + \epsilon
$$ Where: $Y$ = `earnings_med` , $X_r$ = `region`, $X_d$ = `degree`, $X_y$ = `year`, $X_n$ = `count_not_working`, and $X_w$ = `count_working` Using our final model, `earnings_lm`, we performed the following hypotheses testing:

For `region`:

-   $H_0$: $\beta_{r_1} = \beta_{r_2} = \beta_{r_3} = 0$

-   $H_a$: $\beta_{r1} \ne \beta_{r2} \ne \beta_{r3} \ne 0$

For `degree`:

-   $H_0$: $\beta_{d_1} = \beta_{d_2} = 0$

-   $H_a$: $\beta_{d_1} \ne \beta_{d_2} \ne 0$

For `count_working`:

-   $H_0$: $\beta_w = 0$

-   $H_a$: $\beta_w \ne 0$

Using the p-values from the `drop1` function, we see that $\beta_r$, $\beta_g$, and $\beta_w$ are all significant predictors of `earnings_med`.

```{r}
drop1(earnings_lm, test = "F")
```

Furthermore, using the `summary` functions we see that $\beta_{r_1}$, $\beta_{r_2}$, $\beta_{r_3}$, $\beta_{d_1}$, and $\beta_{d_2}$ are all significant predictors of $Y$.
We therefore reject H_0 for both `region` and `degree` and conclude that median income changes based on the regional location of the college and the type of degree the college offers.
Also, we see that there is significant evidence that $\beta_w$ is positive (which confirms our hypothesis in Part 1).
We therefore reject $H_0$ for `count_working` variable and conclude that median earnings tend to increase as the number of working graduates increases.

```{r}
summary(earnings_lm)
```

To further investigate the `region` and `degree` variables, we ran the respective `constrast` functions and found that income significantly varies between all regions as well as between all degree types which confirms our initial hypothesis stated in Part 1.

```{r}
cat("Comparing median income between regions:", "\n")
contrast(emmeans(earnings_lm, ~ region), method = "pairwise", adjust = "none")
cat("\n","Comparing median income between degrees:", "\n", sep = "")
contrast(emmeans(earnings_lm, ~ degree), method = "pairwise", adjust = "none")
```

In conclusion, based on our findings, all of our initial hypotheses seem to be confirmed.
The median earnings do seem to increase with the number of graduates that are able to find a job.
The earnings also vary based on degree type the graduate received and the geographic region of the US where the college is located.
These conclusions do have serious limitations though.
Firstly, our model contained only 5 predictors all of which were found to be significant.
However, the inclusion of more predictors can affect the trends of the model and change the significance of each of the original 5 predictors.
Also, we need to consider the possibility of existence of confounding variables.
For example, it is possible that graduates who go to elite colleges are more likely to both find a job and earn a higher wage.
Along with this, some regions in the US, like the Northeast, tend to have many states with a significantly higher cost of living which can explain the difference in median earnings.
It is also important to account for the fact that we performed multiple tests in this section, hence we adjusted our p-values using the Bonferroni correction.
Firstly, for both region and degree variables, we repeated the pairwise comparisons using the Bonferroni adjusted p-values.
In both cases, our conclusions did not change.

```{r}
cat("Comparing median income between regions:", "\n")
contrast(emmeans(earnings_lm, ~ degree), method = "pairwise", adjust = "bonferroni")
cat("\n", "Comparing median income between degrees:", "\n", sep="")
contrast(emmeans(earnings_lm, ~ region), method = "pairwise", adjust = "bonferroni")
```

Then, since we tested three separate sets of hypotheses, the resulting p-values had to be multiplied by a factor of 3 to perform the Bonferroni correction.
However, in all three cases we ended up with a $p-value < 2 * 10^{-16}$ so it follows that we still must reject $H_0$ in all three cases.

## Robustness of Results

#### Compute Bootstrap Standard Errors

```{r}
earnings_boot <- lm.boot(earnings_lm, R=500,000)
summary(earnings_boot)

## Compute the T-statistic
t_boot <- coef(earnings_lm) / summary(earnings_boot)[["stdev.params"]]
t_orig <- coef(earnings_lm) / summary(earnings_lm)$coefficients[,2]

## Compute the P-values
p_boot <- 2 * pt(abs(t_boot), df = earnings_lm$df.residual, lower.tail = FALSE)
p_orig <- 2 * pt(abs(t_orig), df = earnings_lm$df.residual, lower.tail = FALSE)

## Print T statistics and P-values
print(cbind(`sd-orig` = summary(earnings_lm)$coefficients[,2], `sd-boot` = summary(earnings_boot)[["stdev.params"]], `boot - orig`=summary(earnings_boot)[["stdev.params"]]-summary(earnings_lm)$coefficients[,2], `diff / o (%)`=(summary(earnings_boot)[["stdev.params"]]-summary(earnings_lm)$coefficients[,2])/ summary(earnings_lm)$coefficients[,2] * 100))
print(cbind(`t-orig` = t_orig, `P-orig` = p_orig, `t-boot` = t_boot, `P-boot` = p_boot))
```

All of the coefficients are very closely related and not outside of a magnitude of each other. The largest change was in degree 2, with a percent change of -2.83. Due to the close relation, they appear to suggest no issues. 

#### Leave-one-out Prediction Error

```{r}

LOOR2 <- rsq_loo(earnings_lm)
R2 <- summary(earnings_lm)[["r.squared"]]

print(cbind(`LOOR2`=LOOR2, `R2`=R2))
```

There does not appear to be evidence that the model is overfit. 

#### Check for Influential Points

```{r}

ap <- autoplot(earnings_lm)
ap[4]
# using 
influencePlot(earnings_lm)
ip <- influencePlot(earnings_lm)
hip <- as.numeric(row.names(ip))
earnings_lm$model[hip,]
```

#### Multicolinearity

```{r}
numeric_df <- do.call(data.frame, lapply(earnings_lm$model, FUN = as.numeric.l))
pheatmap(cor(numeric_df), treeheight_col = 0, treeheight_row = 0, display_numbers = TRUE, breaks = seq(-1, 1, length = 101))


vif_values <- vif(earnings_lm)
print(vif_values)

# redoing with better lm

earnings_lm <- lm(earnings_med~region+degree+year+count_not_working, data=scorecard)
numeric_df <- do.call(data.frame, lapply(earnings_lm$model, FUN = as.numeric.l))
pheatmap(cor(numeric_df), treeheight_col = 0, treeheight_row = 0, display_numbers = TRUE, breaks = seq(-1, 1, length = 101))


vif_values <- vif(earnings_lm)
print(vif_values)


```
There is high multicolinearity between count_working and count_not_working. This is also indicated by the high vif for each. Once one of the variables is removed the vifs become more balanced and within 1 of each other. 

## Conclusions

(Your text here)
